# Underwater Scuba Sign Language Detection

A reproducible deepâ€learning framework that automatically recognizes scubaâ€diving hand signals in challenging underwater conditions using the CADDY dataset.  
Built in Google Colab with Fast.ai/PyTorch, it supports scenarioâ€aware splits, ResNetâ€34/50 backbones, advanced augmentations, and exports a serialized Pickle model for direct deployment on Autonomous Underwater Vehicles (AUVs).

---

## ğŸ“– Table of Contents

- [ğŸš€ Features](#-features)  
- [ğŸ—„ï¸ Dataset](#ï¸-dataset)  
- [ğŸ—‚ï¸ Repository Structure](#ï¸-repository-structure)  
- [âš™ï¸ Installation](#ï¸-installation)  
- [ğŸ› ï¸ Data Preparation & Splitting](#ï¸-data-preparation--splitting)  
- [ğŸ§  Model Architecture](#ï¸-model-architecture)  
- [ğŸ‹ï¸ Training Pipeline](#ï¸-training-pipeline)  
- [ğŸ” Evaluation](#ï¸-evaluation)  
- [ğŸ“¦ Export & Deployment](#ï¸-deployment)  
- [ğŸ¤ Contributing](#ï¸-contributing)  

---

## ğŸš€ Features

- **CADDY Underwater Dataset**  
  Uses the publicly available CADDY handâ€gesture dataset (crystalâ€clear, cloudy, dark sequences).  
- **Scenarioâ€Aware Splits**  
  Eight individual scenarios (e.g. `biograd-A/B/C`, `genova-A`, `brodarski-A/B/C/D`) plus an aggregated â€œoverallâ€ split, ensuring no leakage between train/valid/test.  
- **Reproducibility**  
  Deterministic seeding (numpy, PyTorch, Python hash), MD5 checksum verification, and GPU memory monitoring via `nvidia-smi`.  
- **Modular Preprocessing**  
  Python scripts (and Colab notebook) to unzip, validate, list files, and programmatically create splits.  
- **Fast.ai Training**  
  Easy switch between ResNetâ€34 and ResNetâ€50; rich augmentation (rotations, zoom, lighting, warp); callbacks for live training graphs and bestâ€model checkpointing.  
- **Comprehensive Evaluation**  
  â€¢ Top-loss visualization  
  â€¢ Confusion matrices & most-confused class pairs  
  â€¢ Per-scenario accuracy reports  
  â€¢ Exportable CSV/pickle results  
- **Pickle Model Export**  
  Entire trained `Learner` is serialized to a `.pkl` fileâ€”load and infer in any Python environment (e.g. on an AUV).  

---

## ğŸ—„ï¸ Dataset

- **Name:** CADDY Underwater Hand Gesture Dataset  
- **Source:** [CNR-ISSIA CADDY](http://www.caddian.eu)  
- **Structure:**  
all-scenarios/
â”œâ”€ biograd-A/
â”œâ”€ biograd-B/
â”œâ”€ â€¦
â””â”€ brodarski-D/

- **Usage:** Download the ZIP, verify with MD5 = `88dd6fbfc8176d6845dd0f55f95c0c5b`, then unzip.
---
## ğŸ—‚ï¸ Repository Structure
â”œâ”€â”€ data/ # Scripts to download & verify CADDY dataset
â”œâ”€â”€ splits/ # Generated train/valid/test folders per scenario
â”œâ”€â”€ notebooks/ # Google Colab notebook (main pipeline)
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ prepare_data.py # Unzip, MD5-check, directory listing, scenario splits
â”‚ â”œâ”€â”€ train_model.py # Fast.ai training loop (ResNet-34/50, callbacks)
â”‚ â”œâ”€â”€ evaluate_model.py # Interpretation & metrics (top losses, confusion mat.)
â”‚ â””â”€â”€ export_model.py # Serialize Learner to Pickle
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ figures/ # Plots (loss curves, confusion matrices)
â”‚ â”œâ”€â”€ train_valid-df-model-<M>.csv # CSV of train/valid file lists
â”‚ â”œâ”€â”€ test-df-model-<M>.csv # CSV of test file lists
â”‚ â”œâ”€â”€ accuracies-df-model-<M>.csv # Per-scenario accuracy report
â”‚ â””â”€â”€ model-<M>.pkl # Serialized model for deployment
â”œâ”€â”€ requirements.txt # fastai, torchvision, etc.
â””â”€â”€ README.md # This file

---

## âš™ï¸ Installation

```bash
git clone https://github.com/rushabhdhoke/Underwater-scuba-sign-language-Detection.git
cd Underwater-scuba-sign-language-Detection
pip install -r requirements.txt
```

## ğŸ§  Model Architecture
Backbones: ResNet-34 or ResNet-50 (torchvision.models)
Head: Fast.ai default (adaptive pooling â†’ dense â†’ dropout â†’ softmax)
Loss: CrossEntropyLoss
Optimizer: Adam via fit_one_cycle
Augmentations: Rotation, zoom, lighting, warp, normalization to ImageNet stats


## ğŸ‹ï¸ Training Pipeline
Load Data:

```python
from fastai.vision.all import *
data = ImageDataLoaders.from_folder(
    Path('splits'),
    valid_pct=0.2,
    bs=32,
    item_tfms=Resize(224),
    batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)]
)
```
### Create Learner:

```python
learn = cnn_learner(data, resnet50, metrics=accuracy)
```

### Train 
```python
learn.fit_one_cycle(
  10,
  lr_max=1e-3,
  cbs=[ShowGraphCallback(), SaveModelCallback(monitor='accuracy',
                                              fname='resnet50-stage1')]
)
```

### Export

```python
learn.export('results/model-F.pkl')
```


## ğŸ” Evaluation
### Top-Loss Visualization:
```
interp = ClassificationInterpretation.from_learner(learn)
interp.plot_top_losses(9, figsize=(15,11))
```

### Confusion Matrix & Most Confused:
```python
interp.plot_confusion_matrix(figsize=(12,12))
print(interp.most_confused(min_val=2))
```

### Per-Scenario Accuracy:
```python
learn.dls = data_test['genova-A']
preds, targs = learn.get_preds()
acc = accuracy(preds, targs)
```

## ğŸ“¦ Deployment

Load model-F.pkl on your onboard Python stack; feed live frame crops to learner.predict(); trigger AUV actions based on recognized gestures.


## ğŸ¤ Contributing
- Fork the repo
- Create a branch: git checkout -b feature/YourFeature
- Commit: git commit -am 'Add feature'
- Push: git push origin feature/YourFeature
- Open a PR
